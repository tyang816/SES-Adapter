# Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein Language Models

## ðŸš€ Introduction (SES-Adapter)

SES-Adapter, a simple, efficient, and scalable adapter method for enhancing the representation learning of protein language models (PLMs). 

We serialized the protein structure and performed cross-modal-attention with PLM embeddings, effectively improving downstream task performance and convergence efficiency.

<img src="img/framework.png" alt="Logo">

## ðŸ“‘ Results

### Paper Results

We conduct evaluation on 9 state-of-the-art baseline models (**ESM2, ProtBert, ProtT5, Ankh**) across 9 datasets under 4 tasks (**Localization, Function, Solubility, Annotation**).

Results show that compared to vanilla PLMs, SES-Adapter improves downstream task performance by a maximum of **11%** and an average of **3%**, with significantly accelerated training speed by a maximum of **1034%** and an average of **362%,** the convergence rate is also improved by approximately **2** times.

## ðŸ›« Requirement

### Conda Enviroment

Please make sure you have installed **[Anaconda3](https://www.anaconda.com/download)** or **[Miniconda3](https://docs.conda.io/projects/miniconda/en/latest/)**.

```
conda env create -f environment.yaml
conda activate ses_adapter
```

### Hardware

We recommend a **24GB** RTX 3090 or better, but it mainly depends on which PLM you choose.

## ðŸ§¬ Start with SES-Adapter

coming soon..

## ðŸ™Œ Citation

Please cite our work if you have used our code or data.

```

```

